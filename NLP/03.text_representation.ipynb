{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://zhuanlan.zhihu.com/p/42310942\n",
    "            https://zhuanlan.zhihu.com/p/56382372\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP中的文本表示\n",
    "- 基于ONE-HOT, TF-IDF, TEXTRANK等的BAG-OF-WORDS\n",
    "- 主题模型: LSA(SVD), pLSA, LDA;\n",
    "- 基于词向量的固定表征: word2vec, fastText, glove\n",
    "- 基于词向量的动态表征: elmo, GPT, bert\n",
    "### 离散表示\n",
    "##### One-hot\n",
    "词的表示(文本中的一个基元)给每个词编码一个索引,根据索引对每个词进行ont-hot表示.  \n",
    "*特点*: 数据稀疏,维度灾难, 无法表达单词与单词之间的相似程度\n",
    "##### BAG-OF-WORDS\n",
    "(文本/文档/句子)把一个文本表示成向量的形式: 可对文档中的~~每个词~~所有词(有重复)的one-hot表示相加得到. 也叫做计数向量表示.  \n",
    "*特点*: 不考虑词的先后顺序\n",
    "##### Bi-gram和N-gram\n",
    "(文本/文档/句子)与词袋模型类似, 将相邻的两个或者N个词编上索引.  \n",
    "*特点*: 引入了词的顺序, 造成词向量的急剧膨胀, N一般取3 最多不超过5\n",
    "##### TF-IDF\n",
    "对语料库中的一个文档进行表示: 需要统计一个文档中所有词语在语料库中的逆文档频率. 得到文档中每个词语的逆文档频率. (一般会去除停用词)  \n",
    "*特点*: 使用词语的各个单词频率描述一篇文档.  \n",
    "*用途*: 抽取文档关键词. 优点是简单快速, 结果也比较符合实际情况; 缺点是无法体现词的位置信息, 仅以词频衡量词的重要性也不够全面.  \n",
    "*sklearn中的TfidfVectorizer生成TF-IDF特征*:\n",
    "```python\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "​\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                                   ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "```\n",
    "##### 共现矩阵(Cocurrence matrix)\n",
    "如两个词在一句话或者一篇文章中共同出现. 通常我们选择一个距离窗口, 如果窗口宽度为2则认为当前词前后两个词的距离范围内出现的词为共现. 扫描所有文本,统计每个单词的共现词构造共现矩阵(对角矩阵). 对共现矩阵进行PCA或者SVD操作进行降维.  \n",
    "*特点*: 使用稠密向量,计算量大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分布式表示\n",
    "离散表示虽然能够进行词语或者文本的表示, 进而用于情感分析或者文本分类的人物. 但其不能表示词语间的相似程度或者词语间的类比关系. 因此使用分布式表示, 其主要思想是: **一个词的含义可以用它周围的词进行表示, 因为相同上下文语境的词往往会有相似的含义**  \n",
    "#### 静态表征\n",
    "##### NNLM\n",
    "主要目的是为了搭建模型用于*根据前N-1个词预测第N个词的概率*, 其目标函数为:\n",
    "$$ L(\\theta) = \\sum_tlogP(w_t|w_{t-n}, w_{t-n+1},\\dots,w_{t-1})$$\n",
    "使用长度为n-1的滑动窗口遍历整个语料库求和, 使得目标概率最大化,计算量正比于语料库大小,且预测的所有词概率总和应为1.\n",
    "$$ \\sum_{w\\in\\{vocabulary\\}}P(w|w_{t-n+1},\\dots,w_{t-1}) $$\n",
    "NNLM 网络结构如下:  \n",
    "<img src=\"./imgs/NNLM.jpg\" width=\"240\" height=\"240\" align=\"center\"/>  \n",
    "*特点*: 主要目的是为了搭建模型用于预测下一个可能的词, 词向量只是副产物. softmax的计算量太大.\n",
    "#####  word2vec\n",
    "和NNLM相比, word2ec\n",
    "- 取消了隐藏层, 减少了计算量\n",
    "- 采用上下文滑窗而不是前文滑窗\n",
    "- 投影层使用简单的求和平均,而不是拼接向量\n",
    "目标函数为$$ J = \\sum_{w\\in corpus} P(w|context(w)) $$\n",
    "针对输出层需要对语料库中的所有词进行概率计算, 仍然有计算量过大的问题,进行了如下优化:  \n",
    "** hierarchical softmax  \n",
    "层次softmax, 首先统计语料库中各个词出现的频数, 并根据频数对每个词进行哈夫曼编码,构造哈夫曼树  \n",
    "** Negative sampling  \n",
    "\n",
    "<img src=\"./imgs/word2vec.png\" width=\"300\" height=\"240\" align=\"center\"/>  \n",
    "\n",
    "##### fastText  \n",
    "##### glove  \n",
    "#### 动态表征  \n",
    "##### ELMO  \n",
    "##### GPT  \n",
    "##### bert  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}