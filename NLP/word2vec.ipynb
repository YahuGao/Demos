{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def data_helper(s): \n",
    "    # 去除字符串中的数字，字母和中文标点\n",
    "    cn_punctuation = '！？。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.'\n",
    "    s = re.sub(r'[0-9a-zA-Z'+cn_punctuation+']', '', s)\n",
    "    s = re.sub(r'[' + string.punctuation + ']', '', s)\n",
    "    return s\n",
    "s = '你好，9527.欢迎,.!\"\"？‘。。。。'\n",
    "# data_helper(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from tqdm import tqdm\n",
    "def read_corpus(filename='../data/修真四万年.txt'):\n",
    "    '''\n",
    "    contexts: list of list of words(string)\n",
    "    '''\n",
    "    START_TOKEN = '<START>'\n",
    "    END_TOKEN = '<END>'\n",
    "    UNKNOWN_TOKEN = ['<UNKNOWN>']\n",
    "    contexts = [UNKNOWN_TOKEN]\n",
    "    with open(filename) as f:\n",
    "        try:\n",
    "            with tqdm(f.readlines()) as t:\n",
    "                for segment in t:\n",
    "                    if len(segment) == 1:\n",
    "                        continue\n",
    "                    sentences = [sentence + '。'  for sentence in segment.split('。')]\n",
    "                    for sentence in sentences:\n",
    "                        words = jieba.lcut(sentence)\n",
    "                        words = [START_TOKEN] + words + [END_TOKEN]\n",
    "                        contexts.append(words)\n",
    "        except KeyboardInterrupt:\n",
    "            t.close()\n",
    "            raise\n",
    "\n",
    "        t.close()\n",
    "\n",
    "    return contexts\n",
    "\n",
    "contexts = read_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(contexts):\n",
    "    words = [word for line in contexts for word in line]\n",
    "    words = list(set(words))\n",
    "    return words, len(words)\n",
    "\n",
    "vocab, vocab_length = get_vocabulary(contexts)\n",
    "\n",
    "word2idx = {word:idx for idx,word in enumerate(vocab)}\n",
    "idx2word = {idx:word for idx,word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4\n",
    "idx_pairs = []\n",
    "# the first sentence is ['<UNKNOWN>'], start from the sencond sentence\n",
    "cbow_index_pairs = []\n",
    "for sentence in contexts[1:]:\n",
    "    # the first and last word in sentence are '<START>' and '<END>'\n",
    "    for index in range(1, len(sentence)-1):\n",
    "        center_word = sentence[index]\n",
    "        for cursor in range(1, window_size):\n",
    "            if index - cursor > 0:\n",
    "                context_word = sentence[index-cursor]\n",
    "                cbow_index_pairs.append([word2idx[center_word], word2idx[context_word]])\n",
    "            if index + cursor < len(sentence):\n",
    "                context_word = sentence[index+cursor]\n",
    "                cbow_index_pairs.append([word2idx[center_word], word2idx[context_word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_word2vec(index_pairs):\n",
    "    '''\n",
    "    data_pairs: CBOW -- list of (center_word, context_word)  根据上下文预测中心词\n",
    "                Skip-gram -- list of (context_word, center_word)  根据中心词预测目标词\n",
    "    '''\n",
    "    embedding_dims = 5\n",
    "    epoches = 20\n",
    "    lr = 0.001\n",
    "    w1 = Variable(torch.Tensor(embedding_dims, vocab_length).float(), requires_grad=True)\n",
    "    w2 = Variable(torch.Tensor(vocab_length, embedding_dims).float(), requires_grad=True)\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        loss_val = 0\n",
    "        for [center_idx, context_idx] in index_pairs:\n",
    "            x = Variable(torch.zeros(vocab_length)).float()\n",
    "            y = Variable(torch.from_numpy(np.array([context_idx]))).long()\n",
    "            x[center_idx] = 1.0\n",
    "\n",
    "            z1 = torch.matmul(w1, x)\n",
    "            z2 = torch.matmul(w2, z1)\n",
    "\n",
    "            log_softmax = F.log_softmax(z2, dim=0)\n",
    "            loss = F.nll_loss(log_softmax.view(1,-1), y)\n",
    "            loss_val += loss.data\n",
    "            loss.backward()\n",
    "            w1.data -= lr * w1.grad.data\n",
    "            w2.data -= lr * w2.grad.data\n",
    "\n",
    "            w1.grad.data.zero_()\n",
    "            w2.grad.data.zero_()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Loss at epoch {epoch}: {loss_val/len(index_pairs)}')\n",
    "            \n",
    "train_word2vec(cbow_index_pairs[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字词的向量化表示\n",
    "### 1. onehot\n",
    "对于一个大小为N的词典，每个词需要一个由n-1个0和一个1组成的向量。n个词需要一个NxN的二维向量，向量规模比较大，且不能表示词与词之间的关系。\n",
    "### 2. SVD based method\n",
    "遍历数据集中词语的相关数量，组成一个矩阵X，然后对矩阵进行SVD分解$X=USV^T$把$U$作为word embedding。\n",
    "#### 1)基于词-文档的矩阵X\n",
    "假设相关的词会出现在同一个文档中，可以通过遍历数十亿的文档，把出现在一篇文档中的词作为矩阵中一行有效的记录。这会导致矩阵的维度非常大，且随着文档数量的变化而变化\n",
    "#### 2)基于窗口的相关矩阵\n",
    "定义一个滑窗，同时处在滑窗内的词语，认为是相关的，假设有$|V|$个词语，最终组成$|V|X|V|$的相关矩阵。\n",
    "#### 对相关矩阵应用SVD分解有些问题：\n",
    "- 相关矩阵的维度会随着词典大小的变化而变化，\n",
    "- 矩阵非常系数，因为大部分词语是不相关的\n",
    "- 矩阵的维度会比较大，\n",
    "- 执行SVD分解的代价比较大\n",
    "- 词语的出现频率不均衡导致，词语之间的相关性并不准确\n",
    "#### 解决方法有：\n",
    "- 忽略高频词，\n",
    "- 使用带权重的滑窗，距离核心词的距离不同，权重不同\n",
    "- 使用皮尔逊相关系数 设置负数为0。\n",
    "\n",
    "### 3. Iterated based method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
