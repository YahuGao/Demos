{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类模型\n",
    "NB/LR/SVM/LSTM(GRU)/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import jieba\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = './data/'\n",
    "processed_data_file = './data/processed.csv'\n",
    "stopwords_file = './data/stopwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    stopwords=pd.read_csv(stopwords_file,\n",
    "                    index_col=False,quoting=3,sep=\"\\t\",\n",
    "                    names=['stopword'], encoding='utf-8')\n",
    "    stopwords=stopwords['stopword'].values\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(lines):\n",
    "    sentences = []\n",
    "    stopwords = get_stopwords()\n",
    "    for line in lines:\n",
    "        try:\n",
    "            line = line.strip()\n",
    "            segs = jieba.lcut(line)\n",
    "            segs = list(filter(lambda x: len(x) > 1, segs))\n",
    "            segs = list(filter(lambda x: x not in stopwords, segs))\n",
    "            sentences.append(\" \".join(segs))\n",
    "        except Exception as e:\n",
    "            print(line)\n",
    "            print(e)\n",
    "            break\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    df = None\n",
    "    if os.path.exists(processed_data_file):\n",
    "        if os.path.isfile(processed_data_file):\n",
    "            df = pd.read_csv(processed_data_file)\n",
    "        else:\n",
    "            print(\"filename of processed data need to be changed.\")\n",
    "        return df\n",
    "    datasets = []\n",
    "    datafiles = [join(datadir, f) for f in listdir(datadir) if f.endswith('.csv')]\n",
    "    for data_file in datafiles:\n",
    "        category = data_file[7:-9]\n",
    "        df = pd.read_csv(data_file, encoding='utf-8')\n",
    "        df = df.dropna()\n",
    "        lines = df.content.values.tolist()\n",
    "        sentences = text_preprocess(lines)\n",
    "        datasets.extend(list(zip(sentences, [category]*len(sentences))))\n",
    "    random.shuffle(datasets)\n",
    "    contents, labels = zip(*datasets)\n",
    "    df = pd.DataFrame({'content':contents, 'labels':labels})\n",
    "    df.dropna()\n",
    "    df.to_csv('./data/processed.csv')\n",
    "    return df   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Building prefix dict from the default dictionary ...\nLoading model from cache /tmp/jieba.cache\nLoading model cost 0.966 seconds.\nPrefix dict has been built successfully.\n"
    }
   ],
   "source": [
    "df = get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x = df.content.values.tolist()\n",
    "y = df.labels.values.tolist()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n                lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, vocabulary=None)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# 从处理后(降噪)数据中抽取特征 (抽取词袋模型特征)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(\n",
    "    analyzer='word',          # tokenise by character ngrams\n",
    "    max_features=4000         # keep the most common 4000 ngrams\n",
    ")\n",
    "vec.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6269911042899078\n0.6268122960468477\n"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(x_train), y_train)\n",
    "print(classifier.score(vec.transform(x_test), y_test))\n",
    "\n",
    "\n",
    "vec = CountVectorizer(\n",
    "    analyzer='word',          # tokenise by character ngrams\n",
    "    ngram_range=(1,4),        # use ngrams of size 1,2,3\n",
    "    max_features=4000         # keep the most common 4000 ngrams\n",
    ")\n",
    "vec.fit(x_train)\n",
    "classifier.fit(vec.transform(x_train), y_train)\n",
    "print(classifier.score(vec.transform(x_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果可以看出, 虽然使用了ngram, 但是分类器的效果并没有得到提升, 下面我们使用K折交叉验证的方式 对数据进行拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.5311764118935225\n"
    }
   ],
   "source": [
    "# 使用交叉验证\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "def stratifiedKFold_CV(x, y, clf_class, shuffle=True, n_folds=5, **kwargs):\n",
    "    stratifiedk_fold = StratifiedKFold(n_splits=n_folds, shuffle=shuffle)\n",
    "    y_pred = y[:]\n",
    "    for train_index, test_index in stratifiedk_fold.split(x, y):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train = y[train_index]\n",
    "        clf_class.fit(x_train, y_train)\n",
    "        y_pred[test_index] = clf_class.predict(x_test)\n",
    "    return y_pred\n",
    "\n",
    "clf_class = MultinomialNB()\n",
    "print(precision_score(y, stratifiedKFold_CV(vec.transform(x), np.array(y), NB), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier.predict('这 是 有史以来 最 大 的 一 次 军舰 演习'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594758764463",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}