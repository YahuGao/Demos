{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM and Bi-LSTM in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构造LSTM时的参数列表，也就是初始化LSTM层的参数列表\n",
    "\n",
    "    input_size – The number of expected features in the input x       （一个词的词向量的维度）\n",
    "    hidden_size – The number of features in the hidden state h         （存储隐藏状态的矩阵维度）\n",
    "    num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1\n",
    "    nonlinearity – The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh' （非线性激活函数）\n",
    "    bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True    （是否使用偏移量）\n",
    "    batch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
    "    dropout – If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "    bidirectional – If True, becomes a bidirectional RNN. Default: False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必须的包\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 例子\n",
    "num_layers = 1\n",
    "directional = 1\n",
    "rnn = nn.LSTM(input_size=10, hidden_size=20, num_layers=num_layers)       # 每个词向量的维度是10， 隐层特征个数是20， 层数是一层\n",
    "input = torch.randn(5, 4, 10) # 构造输入数据 batchsize是4，意味着提供了4个句子，每个句子有5个词构成，每个词是一个1x10的向量\n",
    "# 初始化hidden state权重short term memory\n",
    "# h_0 of shape (num_layers * num_directions, batch, hidden_size)\n",
    "h0 = torch.randn(1 * 1, 4, 20) \n",
    "# 初始化cell state权重long term memory\n",
    "# c_0 of shape (num_layers * num_directions, batch, hidden_size)\n",
    "c0 = torch.randn(1 * 1, 4, 20)  \n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "brnn = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, bidirectional=True)   # 使用双向LSTM之后 \n",
    "\n",
    "bih0 = torch.randn(2*2, 4, 20)\n",
    "bic0 = torch.randn(2*2, 4, 20)\n",
    "bioutput, (bihn, bicn) = brnn(input, (bih0, bic0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 20])\n",
      "torch.Size([5, 4, 40])\n"
     ]
    }
   ],
   "source": [
    "print(output.size())\n",
    "print(bioutput.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 20])\n",
      "torch.Size([1, 4, 20])\n",
      "torch.Size([4, 4, 20])\n",
      "torch.Size([4, 4, 20])\n"
     ]
    }
   ],
   "source": [
    "print(hn.size())\n",
    "print(cn.size())\n",
    "print(bihn.size())\n",
    "print(bicn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过hidden state和cell state的权重初始化可以看出，训练的时候，其实是训练了batch_size套的权重.\n",
    "而层数和方向成倍增加了短期记忆参数和长期记忆参数的个数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
