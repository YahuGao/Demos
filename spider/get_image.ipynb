{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from contextlib import closing # 用于定义站内用户的Referer\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "view-source:方法，就是看页面源码，并不管动态加载的内容\n",
    "这里面没有图片链接，就说明图片是动态加载的。\n",
    "使用JavaScript动态加载，无外乎两种方式：\n",
    "    外部加载\n",
    "    内部加载\n",
    "外部加载就是在html页面中，以引用的形式，加载一个js，例如这样：\n",
    "<script type=\"text/javascript\" src=\"https://cuijiahua.com/call.js\"></script>\n",
    "这段代码得意思是，引用cuijiahua.com域名下的call.js文件。\n",
    "内部加载就是Javascript脚本内容写在html内.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(content_url):\n",
    "    req = requests.get(url=content_url)\n",
    "    req.encoding = 'utf-8'\n",
    "    html = req.text\n",
    "    bs = BeautifulSoup(html, 'lxml')\n",
    "    list_con_li = bs.find('ul', class_='list_con_li')\n",
    "    chapters = list_con_li.find_all('a')\n",
    "    # print(chapters)\n",
    "    \n",
    "    results = [(chapter['href'], chapter['title'].replace(' ', '_'))\n",
    "               for chapter in chapters]\n",
    "\n",
    "    results = sorted(results, key=lambda x:x[1][4:])\n",
    "    return results\n",
    "\n",
    "target = 'https://www.dmzj.com/info/yaoshenji.html'\n",
    "chapters = get_content(target)\n",
    "print(len(chapters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(chapter_item):\n",
    "    for url, title in chapter_item:\n",
    "        # print(title)\n",
    "        # print(url)\n",
    "        req = requests.get(url=url)\n",
    "        bs = BeautifulSoup(req.text, 'lxml')\n",
    "        script_info = bs.script\n",
    "        pic_numbers = re.findall(r'\\d{13,14}', str(script_info))\n",
    "        try:\n",
    "            suff = re.findall(r'\\|(\\d{5})\\|', str(script_info))[0]\n",
    "            pref = re.findall(r'\\|(\\d{4})\\|', str(script_info))[0]\n",
    "        except:\n",
    "            print(url)\n",
    "            print(title)\n",
    "            exit()\n",
    "        prefix = 'https://images.dmzj.com/img/chapterpic/'\n",
    "        pic_urls_order = []\n",
    "        for pic_number in pic_numbers:\n",
    "            if len(pic_number) == 13:\n",
    "                pic_number_index = pic_number + '0'\n",
    "            else:\n",
    "                pic_number_index = pic_number\n",
    "                \n",
    "            pic_url = prefix + pref + '/' + suff + '/' + pic_number + '.jpg'\n",
    "            pic_urls_order.append((pic_number_index, pic_url))\n",
    "        \n",
    "        pic_urls_order = sorted(pic_urls_order, key=lambda x:x[0])\n",
    "        \n",
    "        download_header = {'Referer':url}\n",
    "        if not os.path.exists(title):\n",
    "            os.mkdir(title)\n",
    "        for item in range(len(pic_urls_order)):\n",
    "            url = pic_urls_order[item][1]\n",
    "            # print(url)\n",
    "            with closing(requests.get(url, headers=download_header,\n",
    "                                     stream=True)) as response:\n",
    "                chunk_size = 1024\n",
    "                # print(response.headers)\n",
    "                content_size = int(response.headers['content-length'])\n",
    "                if response.status_code == 200:\n",
    "                    with open(title + '/' + str(item) + '.jpg', 'wb') as file:\n",
    "                        # print('file size: %.2f KB' % (content_size / chunk_size))\n",
    "                        for data in response.iter_content(chunk_size=chunk_size):\n",
    "                            file.write(data)\n",
    "                else:\n",
    "                    print('link error')\n",
    "                    print(response.status_code)\n",
    "                    return 1\n",
    "        \n",
    "    return 0\n",
    "            \n",
    "\n",
    "# get_images(chapters[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "————————————————\n",
    "版权声明：本文为CSDN博主「行者小朱」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "原文链接：https://blog.csdn.net/u012050154/article/details/80032072\n",
    "'''\n",
    "class MyThread(threading.Thread):\n",
    "    def __init__(self, func, args=()):\n",
    "        super(MyThread, self).__init__()\n",
    "        self.func = func\n",
    "        self.args = args\n",
    " \n",
    "    def run(self):\n",
    "        self.result = self.func(self.args)\n",
    " \n",
    "    def get_result(self):\n",
    "        threading.Thread.join(self) # 等待线程执行完毕\n",
    "        try:\n",
    "            return self.result\n",
    "        except Exception:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "threads_num = 10\n",
    "chapter_length = len(chapters)\n",
    "step = chapter_length // threads_num\n",
    "print(step)\n",
    "tasks = []\n",
    "start = end = 0\n",
    "for item in range(threads_num):\n",
    "    start = end\n",
    "    end += step\n",
    "    if end > chapter_length:\n",
    "        end = chapter_length\n",
    "    task = MyThread(get_images, (chapters[start:end]))\n",
    "    task.start()\n",
    "    tasks.append(task)\n",
    "    \n",
    "\n",
    "for task in tasks:\n",
    "    print(task.get_result(), end='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from https://blog.csdn.net/rankun1/article/details/81357179\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    " \n",
    "# 通过find定位标签\n",
    "# BeautifulSoup文档：https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html\n",
    "def bs_parse_movies(html):\n",
    "    movie_list = []\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    # 查找所有class属性为hd的div标签\n",
    "    div_list = soup.find_all('div', class_='hd')\n",
    "    # 获取每个div中的a中的span（第一个），并获取其文本\n",
    "    for each in div_list:\n",
    "        movie = each.a.span.text.strip()\n",
    "        movie_list.append(movie)\n",
    " \n",
    "    return movie_list\n",
    " \n",
    "# css选择器定位标签\n",
    "# 更多ccs选择器语法：http://www.w3school.com.cn/cssref/css_selectors.asp\n",
    "# 注意：BeautifulSoup并不是每个语法都支持\n",
    "def bs_css_parse_movies(html):\n",
    "    movie_list = []\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    # 查找所有class属性为hd的div标签下的a标签的第一个span标签\n",
    "    div_list = soup.select('div.hd > a > span:nth-of-type(1)')\n",
    "    # 获取每个span的文本\n",
    "    for each in div_list:\n",
    "        movie = each.text.strip()\n",
    "        movie_list.append(movie)\n",
    " \n",
    "    return movie_list\n",
    " \n",
    "# XPATH定位标签\n",
    "# 更多xpath语法：https://blog.csdn.net/gongbing798930123/article/details/78955597\n",
    "def xpath_parse_movies(html):\n",
    "    et_html = etree.HTML(html)\n",
    "    # 查找所有class属性为hd的div标签下的a标签的第一个span标签\n",
    "    urls = et_html.xpath(\"//div[@class='hd']/a/span[1]\")\n",
    " \n",
    "    movie_list = []\n",
    "    # 获取每个span的文本\n",
    "    for each in urls:\n",
    "        movie = each.text.strip()\n",
    "        movie_list.append(movie)\n",
    " \n",
    "    return movie_list\n",
    " \n",
    "def get_movies():\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.82 Safari/537.36',\n",
    "        'Host': 'movie.douban.com'\n",
    "    }\n",
    " \n",
    "    link = 'https://movie.douban.com/top250'\n",
    "    r = requests.get(link, headers=headers, timeout=10)\n",
    "    print(\"响应状态码:\", r.status_code)\n",
    "    if 200 != r.status_code:\n",
    "        return None\n",
    " \n",
    "    # 三种定位元素的方式：\n",
    " \n",
    "    # 普通BeautifulSoup find\n",
    "    return bs_parse_movies(r.text)\n",
    "    # BeautifulSoup css select\n",
    "    return bs_css_parse_movies(r.text)\n",
    "    # xpath\n",
    "    return xpath_parse_movies(r.text)\n",
    " \n",
    "movies = get_movies()\n",
    "print(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
