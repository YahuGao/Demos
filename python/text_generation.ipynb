{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 如何生成序列数据\n",
    "将循环神经网络用于生成序列数据. 使用前面的标记作为输入, 训练一个网络来预测序列中接下来的一个或多个标记. 标记通常是单词或字符, 给定前面的标记, 能够对下一个标记的概率进行建模的任何网络都叫做语言模型. 语言模型能够捕捉到语言的潜在空间, 即语言的统计结构.\n",
    "一旦训练好了这样一个语言模型, 就可以从中采样(sample, 即生成新序列). 向模型中输入一个初始文本字符串[即条件数据], 要求模型生成下一个字符或者下一个单词, 然后将生成的输出添加到输入数据中去. 多次重复这一过程, 这个循环可以生成任意长度的序列, 该序列反映了模型训练数据的结构.\n",
    "##### 采样策略的重要性\n",
    "生成文本时,如何选择下一个字符至关重要, 一种简单的方法叫贪婪采样(greedy sampling), 即始终选择概率最大的字符, 这种方法会得到重复的, 可预测的字符; 另一种方法是在采样的过程中引入随机性, 从下一个字符的概率分布中进行采样, 这叫做随机采样(stochastic samplit). 随机采样虽然能够表现出创造性, 但是在采样的过程中无法控制随机性的大小.  \n",
    "为了控制采样过程中随机性的大小, 我们引入了一个叫作softmax温度的参数, 用于表示采样分布的熵, 即表示所选的下一个字符会有多么的出人意料或者多么的可预测. 给定一个temperature值,对softmax的输出进行重新加权, 计算得到一个新的概率分布."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# original_distriburion: 概率值组成的一维Numpy数组, 概率值之和必须等于1\n",
    "# temperature: 用于定量描述输出分布的熵\n",
    "def reweight_distribution(original_distriburion, temperature=0.5):\n",
    "    distribution = np.log(original_distriburion) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    # 返回原始分布重新加权后的结果, distribution的求和肯呢个不再等于1\n",
    "    # 因此, 将它除以求和一得到新的分布\n",
    "    return distribution / np.sum(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "更高的温度得到的是熵更大的采样分布, 会生成更加出人意料的, 更加无结构的生成数据; 更小的温度值对应更小的随机性, 以及更加可预测的生成数据."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 字符级的LSTM文本生成\n",
    "针对尼采的写作风格和主题的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get data\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract data\n",
    "# 提取60个字符组成的序列\n",
    "maxlen = 60\n",
    "# 每三个字符采样一个新序列\n",
    "step = 3\n",
    "# 保存所提取的序列\n",
    "sentences = []\n",
    "# 保存目标(即下一个字符)\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i+maxlen])\n",
    "    next_chars.append(text[i+maxlen])\n",
    "\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "chars = sorted(list(set(text))) # list of unique chars in corpurs\n",
    "print('Unique characters:', len(chars))\n",
    "\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build network\n",
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "目标是one-hot编码, 所以训练模型需要使用categorical_crossentropy作为损失."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 训练语言模型并从中采样\n",
    "给定一个训练好的模型和一个种子文本片段, 我们可以通过重复以下操作来生成新的文本:\n",
    "- 给定目前已生成的文本, 从模型中得到下一个字符的概率分布.\n",
    "- 根据某个温度对分布进行重新加权\n",
    "- 根据重新加权的分布对下一个字符进行随机采样\n",
    "- 将新字符添加到文本末尾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "文本生成循环, 反复训练并生成文本, 每轮过后都使用一系列不同的温度值来生成文本."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 60):\n",
    "    print('epoch', epoch)\n",
    "    model.fit(x, y, batch_size=128, epochs=1)\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices]\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "较小的温度值会得到极端重复和可预测的文本, 但局部结构是非常真实的, 特别是所有的单词都是真实的英文单词. 随着温度值越来越大, 生成的文本更加出人意料, 更加具有创造性, 会出现全新的单词. 对于较大的温度值, 大部分的单词看起来像是半随机的字符串."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "小结:\n",
    "- 我们可以生成离散的序列数据: 给定前面的标记, 训练模型来预测接下来的一个或多个标记.\n",
    "- 对于文本来说, 这种模型叫做语言模型. 它可以是单词级的,也可以是字符级的\n",
    "- 对下一个标记进行采样, 需要在坚持模型的判断与引入随机性之间寻找平衡.\n",
    "- 使用softmax温度, 一定要尝试多种不同的温度, 以找到合适的一个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TODO: DeepDream"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/home/yahu/anaconda3/envs/ml/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "text_generatiation.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
